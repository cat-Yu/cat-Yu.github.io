---
---

@string{aps = {American Physical Society,}}

@inproceedings{Ring-a-Pose,
author={Yu, Tianhong Catherine and Hu, Guilin and Zhang, Ruidong and Lim, Hyunchul and Mahmud, Saif and Lee, Chi-Jung and Li, Ke and Agarwal, Devansh and Nie, Shuyang and Oh, Jinseok and Guimbretiere, Francois and Zhang, Cheng},
title = {Ring-a-Pose: A Ring for Continuous Hand Pose Tracking},
year = {2024},
selected={true},
doi = {https://doi.org/10.1145/3699741},
abstract = {We present Ring-a-Pose, a single untethered ring that tracks continuous 3D hand poses. Located in the center of the hand, the ring emits an inaudible acoustic signal that each hand pose reflects differently. Ring-a-Pose imposes minimal obtrusions on the hand, unlike multi-ring or glove systems. It is not affected by the choice of clothing that may cover wrist-worn systems. In a series of three user studies with a total of 36 participants, we evaluate Ring-a-Pose’s performance on pose tracking and micro-finger gesture recognition. Without collecting any training data from a user, Ring-a-Pose tracks continuous hand poses with a joint error of 14.1mm. The joint error decreases to 10.3mm for fine-tuned user-dependent models. Ring-a-Pose recognizes 7-class micro-gestures with a 90.60% and 99.27% accuracy for user-independent and user-dependent models, respectively. Furthermore, the ring exhibits promising performance when worn on any finger. Ring-a-Pose enables the future of smart rings to track and recognize hand poses using relatively low-power acoustic sensing.},
booktitle = {IMWUT},
video = {https://youtu.be/j8S0u9Igebc?si=gUAU4DWdl7fO-CnE},
preview = {ring-a-pose-thumbnail.png}
}
@inproceedings{SeamPose,
author={Yu, Tianhong Catherine and Zhang*, Manru Mary and He*, Peter and Lee, Chi-Jung and Cheesman, Cassidy and Mahmud, Saif and Zhang, Ruidong and Guimbretiere, Francois and Zhang, Cheng},
title = {SeamPose: Repurposing Seams as Capacitive Sensors in a Shirt for Upper-Body Pose Tracking},
year = {2024},
selected={true},
doi = {https://dl.acm.org/doi/10.1145/3654777.3676341},
abstract = {Seams are areas of overlapping fabric formed by stitching two or more pieces of fabric together in the cut-and-sew apparel manufacturing process. In SeamPose, we repurposed seams as capacitive sensors in a shirt for continuous upper-body pose estimation. Compared to previous all-textile motion-capturing garments that place the electrodes on the clothing surface, our solution leverages existing seams inside of a shirt by machine-sewing insulated conductive threads over the seams. The unique invisibilities and placements of the seams afford the sensing shirt to look and wear similarly as a conventional shirt while providing exciting pose-tracking capabilities. To validate this approach, we implemented a proof-of-concept untethered shirt with 8 capacitive sensing seams. With a 12-participant user study, our customized deep-learning pipeline accurately estimates the relative (to the pelvis) upper-body 3D joint positions with a mean per joint position error (MPJPE) of 6.0 cm. SeamPose represents a step towards unobtrusive integration of smart clothing for everyday pose estimation.},
booktitle = {UIST},
video = {https://youtu.be/KBBBOOAVwes},
preview = {SeamPoseThumbnail.png}
}

@inproceedings{EchoWrist,
author = {Lee*, Chi-Jung and Zhang*, Ruidong and Agarwal, Devansh and Yu, Tianhong Catherine and Gunda, Vipin and Lopez, Oliver and Kim, James and Yin, Sicheng and Dong, Boao and Li, Ke and Sakashita, Mose and Guimbretiere, Francois and Zhang, Cheng},
title = {EchoWrist: Continuous Hand Pose Tracking and Hand-Object Interaction Recognition Using Low-Power Active Acoustic Sensing On a Wristband},
year = {2024},
doi = {https://doi.org/10.1145/3613904.3642910},
abstract = {Our hands serve as a fundamental means of interaction with the world around us. Therefore, understanding hand poses and interaction contexts is critical for human-computer interaction (HCI). We present EchoWrist, a low-power wristband that continuously estimates 3D hand poses and recognizes hand-object interactions using active acoustic sensing. EchoWrist is equipped with two speakers emitting inaudible sound waves toward the hand. These sound waves interact with the hand and its surroundings through reflections and diffractions, carrying rich information about the hand’s shape and the objects it interacts with. The information captured by the two microphones goes through a deep learning inference system that recovers hand poses and identifies various everyday hand activities. Results from the two 12-participant user studies show that EchoWrist is effective and efficient at tracking 3D hand poses and recognizing hand-object interactions. Operating at 57.9 mW, EchoWrist can continuously reconstruct 20 3D hand joints with MJEDE of 4.81 mm and recognize 12 naturalistic hand-object interactions with 97.6\% accuracy.},
booktitle = {CHI},
video = {https://youtu.be/yCLDzKarIEo?si=clRCOeTx818PJtPK},
preview = {EchoWrist.png}
}

@inproceedings{Skinergy,
  title={Skinergy: Machine-Embroidered Silicone-Textile Composites as On-Skin Self-Powered Input Sensors},
  author={Yu, Tianhong Catherine and Wang*, Nancy and Ellenbogen*, Sarah and Kao, Cindy Hsin-Liu},
  booktitle={UIST},
  pages={1--15},
  year={2023},
  selected={true},
  abstract={We propose Skinergy for self-powered on-skin input sensing, a step towards prolonged on-skin device usages. In contrast to prior on-skin gesture interaction sensors, Skinergy’s sensor operation does not require external power. Enabled by the triboelectric nanogenerator (TENG) phenomenon, the machine-embroidered silicone-textile composite sensor converts mechanical energy from the input interaction into electrical energy. Our proof-of-concept untethered sensing system measures the voltages of generated electrical signals which are then processed for a diverse set of sensing tasks: discrete touch detection, multi-contact detection, contact localization, and gesture recognition. Skinergy is fabricated with off-the-shelf materials. The aesthetic and functional designs can be easily customized and digitally fabricated. We characterize Skinergy and conduct a 10-participant user study to (1) evaluate its gesture recognition performance and (2) probe user perceptions and potential applications.  Skinergy achieves 92.8 percent accuracy for a 11-class gesture recognition task. Our findings reveal that human factors (e.g., individual differences in skin properties, and aesthetic preferences) are key considerations in designing self-powered on-skin sensors for human inputs.},
  doi={https://doi.org/10.1145/3586183.3606729},
  website={https://www.hybridbody.human.cornell.edu/skinergy},
  video={https://youtu.be/ZXRZnPM9Rfc},
  preview={SkinergyThumbnail.png}
}

@INPROCEEDINGS{RobotSweater,
  author={Si*, Zilin and Yu*, Tianhong Catherine and Morozov, Katrene and McCann, James and Yuan, Wenzhen},
  booktitle={ICRA}, 
  title={RobotSweater: Scalable, Generalizable, and Customizable Machine-Knitted Tactile Skins for Robots}, 
  year={2023},
  volume={},
  number={},
  pages={10352-10358},
  selected={true},
  abstract={Tactile sensing is essential for robots to perceive and react to the environment. However, it remains a challenge to make large-scale and flexible tactile skins on robots. Industrial machine knitting provides solutions to manufacture customiz-able fabrics. Along with functional yarns, it can produce highly customizable circuits that can be made into tactile skins for robots. In this work, we present RobotSweater, a machine-knitted pressure-sensitive tactile skin that can be easily applied on robots. We design and fabricate a parameterized multi-layer tactile skin using off-the-shelf yarns, and characterize our sensor on both a flat testbed and a curved surface to show its robust contact detection, multi-contact localization, and pressure sensing capabilities. The sensor is fabricated using a well-established textile manufacturing process with a programmable industrial knitting machine, which makes it highly customizable and low-cost. The textile nature of the sensor also makes it easily fit curved surfaces of different robots and have a friendly appearance. Using our tactile skins, we conduct closed-loop control with tactile feedback for two applications: (1) human lead-through control of a robot arm, and (2) human-robot interaction with a mobile robot.},
  doi={https://doi.org/10.1109/ICRA48891.2023.10161321},
  website={https://labs.ri.cmu.edu/robotouch/robotsweater/},
  video={https://youtu.be/M1OFmqBRIB8},
  preview={RobotSweaterThumbnail.png}
}

@inproceedings{uKnit,
  title={uKnit: A Position-Aware Reconfigurable Machine-Knitted Wearable for Gestural Interaction and Passive Sensing using Electrical Impedance Tomography},
  author={Yu, Tianhong Catherine and Arakawa, Riku and McCann, James and Goel, Mayank},
  booktitle={CHI},
  pages={1--17},
  selected={true},
  year={2023},
  abstract={A scarf is inherently reconfigurable: wearers often use it as a neck wrap, a shawl, a headband, a wristband, and more. We developed uKnit, a scarf-like soft sensor with scarf-like reconfigurability, built with machine knitting and electrical impedance tomography sensing. Soft wearable devices are comfortable and thus attractive for many human-computer interaction scenarios. While prior work has demonstrated various soft wearable capabilities, each capability is device- and location-specific, being incapable of meeting users’ various needs with a single device. In contrast, uKnit explores the possibility of one-soft-wearable-for-all. We describe the fabrication and sensing principles behind uKnit, demonstrate several example applications, and evaluate it with 10-participant user studies and a washability test. uKnit achieves 88.0percent/78.2percent accuracy for 5-class worn-location detection and 80.4percent/75.4percent accuracy for 7-class gesture recognition with a per-user/universal model. Moreover, it identifies respiratory rate with an error rate of 1.25 bpm and detects binary sitting postures with an average accuracy of 86.2 percent.},
  doi={https://doi.org/10.1145/3544548.3580692},
  website={https://smashlab.io/publications/uknit/},
  video={https://youtu.be/uPu7JEenUWk},
  preview={uKnitThumbnail.png}
}


@inproceedings{KnitoutVisualizer,
  title={Coupling Programs and Visualization for Machine Knitting},
  author={Yu, Tianhong Catherine and McCann, James},
  booktitle={SCF},
  pages={1--10},
  year={2020},
  abstract={To effectively program knitting machines, like any fabrication machine, users must be able to place the code they write in correspondence with the output the machine produces. This mapping is used in the code-to-output direction to understand what their code will produce, and in the output-to-code direction to debug errors in the finished product. In this paper, we describe and demonstrate an interface that provides two-way coupling between high- or low-level knitting code and a topological visualization of the knitted output. Our system allows the user to locate the knitting machine operations generated by any selected code, as well as the code that generates any selected knitting machine operation. This link between the code and visualization has the potential to reduce the time spent in design, implementation, and debugging phases, and save material costs by catching errors before actually knitting the object. We show examples of patterns designed using our tool and describe common errors that the tool catches when used in an academic lab setting and an undergraduate course.},
  doi={https://doi.org/10.1145/3424630.3425410},
  website={https://textiles-lab.github.io/publications/2020-knitout-visualizer/},
  video={https://youtu.be/SCjqPyj9WUc},
  try={https://textiles-lab.github.io/knitout-live-visualizer/},
  code={https://github.com/textiles-lab/knitout-live-visualizer},
  preview={knitoutVisualizerThumbnail.jpeg}
}