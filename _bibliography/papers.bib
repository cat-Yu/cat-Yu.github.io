---
---

@string{aps = {American Physical Society,}}

@inproceedings{Skinergy,
  title={Skinergy: Machine-Embroidered Silicone-Textile Composites as On-Skin Self-Powered Input Sensors},
  author={Yu, Tianhong Catherine and Wang*, Nancy and Ellenbogen*, Sarah and Kao, Cindy Hsin-Liu},
  booktitle={Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
  pages={1--15},
  year={2023},
  selected={true},
  abstract={We propose Skinergy for self-powered on-skin input sensing, a step towards prolonged on-skin device usages. In contrast to prior on-skin gesture interaction sensors, Skinergy’s sensor operation does not require external power. Enabled by the triboelectric nanogenerator (TENG) phenomenon, the machine-embroidered silicone-textile composite sensor converts mechanical energy from the input interaction into electrical energy. Our proof-of-concept untethered sensing system measures the voltages of generated electrical signals which are then processed for a diverse set of sensing tasks: discrete touch detection, multi-contact detection, contact localization, and gesture recognition. Skinergy is fabricated with off-the-shelf materials. The aesthetic and functional designs can be easily customized and digitally fabricated. We characterize Skinergy and conduct a 10-participant user study to (1) evaluate its gesture recognition performance and (2) probe user perceptions and potential applications.  Skinergy achieves 92.8 percent accuracy for a 11-class gesture recognition task. Our findings reveal that human factors (e.g., individual differences in skin properties, and aesthetic preferences) are key considerations in designing self-powered on-skin sensors for human inputs.},
  doi={https://doi.org/10.1145/3586183.3606729},
  website={https://www.hybridbody.human.cornell.edu/skinergy},
  video={https://youtu.be/ZXRZnPM9Rfc},
  preview={SkinergyThumbnail.png}
}

@INPROCEEDINGS{RobotSweater,
  author={Si*, Zilin and Yu*, Tianhong Catherine and Morozov, Katrene and McCann, James and Yuan, Wenzhen},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={RobotSweater: Scalable, Generalizable, and Customizable Machine-Knitted Tactile Skins for Robots}, 
  year={2023},
  volume={},
  number={},
  pages={10352-10358},
  selected={true},
  abstract={Tactile sensing is essential for robots to perceive and react to the environment. However, it remains a challenge to make large-scale and flexible tactile skins on robots. Industrial machine knitting provides solutions to manufacture customiz-able fabrics. Along with functional yarns, it can produce highly customizable circuits that can be made into tactile skins for robots. In this work, we present RobotSweater, a machine-knitted pressure-sensitive tactile skin that can be easily applied on robots. We design and fabricate a parameterized multi-layer tactile skin using off-the-shelf yarns, and characterize our sensor on both a flat testbed and a curved surface to show its robust contact detection, multi-contact localization, and pressure sensing capabilities. The sensor is fabricated using a well-established textile manufacturing process with a programmable industrial knitting machine, which makes it highly customizable and low-cost. The textile nature of the sensor also makes it easily fit curved surfaces of different robots and have a friendly appearance. Using our tactile skins, we conduct closed-loop control with tactile feedback for two applications: (1) human lead-through control of a robot arm, and (2) human-robot interaction with a mobile robot.},
  doi={https://doi.org/10.1109/ICRA48891.2023.10161321},
  website={https://labs.ri.cmu.edu/robotouch/robotsweater/},
  video={https://youtu.be/M1OFmqBRIB8},
  preview={RobotSweaterThumbnail.png}
}

@inproceedings{uKnit,
  title={uKnit: A Position-Aware Reconfigurable Machine-Knitted Wearable for Gestural Interaction and Passive Sensing using Electrical Impedance Tomography},
  author={Yu, Tianhong Catherine and Arakawa, Riku and McCann, James and Goel, Mayank},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--17},
  selected={true},
  year={2023},
  abstract={A scarf is inherently reconfigurable: wearers often use it as a neck wrap, a shawl, a headband, a wristband, and more. We developed uKnit, a scarf-like soft sensor with scarf-like reconfigurability, built with machine knitting and electrical impedance tomography sensing. Soft wearable devices are comfortable and thus attractive for many human-computer interaction scenarios. While prior work has demonstrated various soft wearable capabilities, each capability is device- and location-specific, being incapable of meeting users’ various needs with a single device. In contrast, uKnit explores the possibility of one-soft-wearable-for-all. We describe the fabrication and sensing principles behind uKnit, demonstrate several example applications, and evaluate it with 10-participant user studies and a washability test. uKnit achieves 88.0percent/78.2percent accuracy for 5-class worn-location detection and 80.4percent/75.4percent accuracy for 7-class gesture recognition with a per-user/universal model. Moreover, it identifies respiratory rate with an error rate of 1.25 bpm and detects binary sitting postures with an average accuracy of 86.2 percent.},
  doi={https://doi.org/10.1145/3544548.3580692},
  website={https://smashlab.io/publications/uknit/},
  video={https://youtu.be/uPu7JEenUWk},
  preview={uKnitThumbnail.png}
}


@inproceedings{KnitoutVisualizer,
  title={Coupling programs and visualization for machine knitting},
  author={Yu, Tianhong Catherine and McCann, James},
  booktitle={Proceedings of the 5th Annual ACM Symposium on Computational Fabrication},
  pages={1--10},
  year={2020},
  abstract={To effectively program knitting machines, like any fabrication machine, users must be able to place the code they write in correspondence with the output the machine produces. This mapping is used in the code-to-output direction to understand what their code will produce, and in the output-to-code direction to debug errors in the finished product. In this paper, we describe and demonstrate an interface that provides two-way coupling between high- or low-level knitting code and a topological visualization of the knitted output. Our system allows the user to locate the knitting machine operations generated by any selected code, as well as the code that generates any selected knitting machine operation. This link between the code and visualization has the potential to reduce the time spent in design, implementation, and debugging phases, and save material costs by catching errors before actually knitting the object. We show examples of patterns designed using our tool and describe common errors that the tool catches when used in an academic lab setting and an undergraduate course.},
  doi={https://doi.org/10.1145/3424630.3425410},
  website={https://textiles-lab.github.io/publications/2020-knitout-visualizer/},
  video={https://youtu.be/SCjqPyj9WUc},
  try={https://textiles-lab.github.io/knitout-live-visualizer/},
  code={https://github.com/textiles-lab/knitout-live-visualizer},
  preview={knitoutVisualizerThumbnail.jpeg}
}