<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Catherine Yu 于天泓</title> <meta name="author" content="Catherine Tianhong Yu"> <meta name="description" content="\* equal contribution"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?ebb54d8b4c049703faa046e95fe91f77"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A6%84&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="catherineyu.com/publications/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Catherine Yu 于天泓</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/littlething/">littlethings</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">\* equal contribution</p> </header> <article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/SeamPoseThumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/SeamPoseThumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/SeamPoseThumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/SeamPoseThumbnail.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="SeamPoseThumbnail.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="SeamPose" class="col-sm-8"> <div class="title">SeamPose: Repurposing Seams as Capacitive Sensors in a Shirt for Upper-Body Pose Tracking</div> <div class="author"> <em>Tianhong Catherine Yu</em>, Manru Mary Zhang*, Peter He*, <a href="https://cjlisalee.github.io/" rel="external nofollow noopener" target="_blank">Chi-Jung Lee</a>, Cassidy Cheesman, <a href="https://saif-mahmud.github.io/" rel="external nofollow noopener" target="_blank">Saif Mahmud</a>, <a href="https://ruidongzhang.com/" rel="external nofollow noopener" target="_blank">Ruidong Zhang</a>, <a href="https://www.cs.cornell.edu/~francois/" rel="external nofollow noopener" target="_blank">Francois Guimbretiere</a>, and <a href="https://czhang.org/" rel="external nofollow noopener" target="_blank">Cheng Zhang</a> </div> <div class="periodical"> <em>In UIST</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/3654777.3676341" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://youtu.be/KBBBOOAVwes" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">video</a> </div> <div class="abstract hidden"> <p>Seams are areas of overlapping fabric formed by stitching two or more pieces of fabric together in the cut-and-sew apparel manufacturing process. In SeamPose, we repurposed seams as capacitive sensors in a shirt for continuous upper-body pose estimation. Compared to previous all-textile motion-capturing garments that place the electrodes on the clothing surface, our solution leverages existing seams inside of a shirt by machine-sewing insulated conductive threads over the seams. The unique invisibilities and placements of the seams afford the sensing shirt to look and wear similarly as a conventional shirt while providing exciting pose-tracking capabilities. To validate this approach, we implemented a proof-of-concept untethered shirt with 8 capacitive sensing seams. With a 12-participant user study, our customized deep-learning pipeline accurately estimates the relative (to the pelvis) upper-body 3D joint positions with a mean per joint position error (MPJPE) of 6.0 cm. SeamPose represents a step towards unobtrusive integration of smart clothing for everyday pose estimation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/EchoWrist-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/EchoWrist-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/EchoWrist-1400.webp"></source> <img src="/assets/img/publication_preview/EchoWrist.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="EchoWrist.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="EchoWrist" class="col-sm-8"> <div class="title">EchoWrist: Continuous Hand Pose Tracking and Hand-Object Interaction Recognition Using Low-Power Active Acoustic Sensing On a Wristband</div> <div class="author"> <a href="https://cjlisalee.github.io/" rel="external nofollow noopener" target="_blank">Chi-Jung Lee*</a>, <a href="https://ruidongzhang.com/" rel="external nofollow noopener" target="_blank">Ruidong Zhang*</a>, Devansh Agarwal, <em>Tianhong Catherine Yu</em>, Vipin Gunda, Oliver Lopez, James Kim, Sicheng Yin, Boao Dong, <a href="https://keli97.github.io/" rel="external nofollow noopener" target="_blank">Ke Li</a>, Mose Sakashita, <a href="https://www.cs.cornell.edu/~francois/" rel="external nofollow noopener" target="_blank">Francois Guimbretiere</a>, and <a href="https://czhang.org/" rel="external nofollow noopener" target="_blank">Cheng Zhang</a> </div> <div class="periodical"> <em>In CHI</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3613904.3642910" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://youtu.be/yCLDzKarIEo?si=clRCOeTx818PJtPK" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">video</a> </div> <div class="abstract hidden"> <p>Our hands serve as a fundamental means of interaction with the world around us. Therefore, understanding hand poses and interaction contexts is critical for human-computer interaction (HCI). We present EchoWrist, a low-power wristband that continuously estimates 3D hand poses and recognizes hand-object interactions using active acoustic sensing. EchoWrist is equipped with two speakers emitting inaudible sound waves toward the hand. These sound waves interact with the hand and its surroundings through reflections and diffractions, carrying rich information about the hand’s shape and the objects it interacts with. The information captured by the two microphones goes through a deep learning inference system that recovers hand poses and identifies various everyday hand activities. Results from the two 12-participant user studies show that EchoWrist is effective and efficient at tracking 3D hand poses and recognizing hand-object interactions. Operating at 57.9 mW, EchoWrist can continuously reconstruct 20 3D hand joints with MJEDE of 4.81 mm and recognize 12 naturalistic hand-object interactions with 97.6% accuracy.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/SkinergyThumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/SkinergyThumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/SkinergyThumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/SkinergyThumbnail.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="SkinergyThumbnail.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Skinergy" class="col-sm-8"> <div class="title">Skinergy: Machine-Embroidered Silicone-Textile Composites as On-Skin Self-Powered Input Sensors</div> <div class="author"> <em>Tianhong Catherine Yu</em>, Nancy Wang*, Sarah Ellenbogen*, and <a href="https://www.hybridbody.human.cornell.edu/cindyhlkao" rel="external nofollow noopener" target="_blank">Cindy Hsin-Liu Kao</a> </div> <div class="periodical"> <em>In UIST</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3586183.3606729" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://youtu.be/ZXRZnPM9Rfc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">video</a> <a href="https://www.hybridbody.human.cornell.edu/skinergy" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>We propose Skinergy for self-powered on-skin input sensing, a step towards prolonged on-skin device usages. In contrast to prior on-skin gesture interaction sensors, Skinergy’s sensor operation does not require external power. Enabled by the triboelectric nanogenerator (TENG) phenomenon, the machine-embroidered silicone-textile composite sensor converts mechanical energy from the input interaction into electrical energy. Our proof-of-concept untethered sensing system measures the voltages of generated electrical signals which are then processed for a diverse set of sensing tasks: discrete touch detection, multi-contact detection, contact localization, and gesture recognition. Skinergy is fabricated with off-the-shelf materials. The aesthetic and functional designs can be easily customized and digitally fabricated. We characterize Skinergy and conduct a 10-participant user study to (1) evaluate its gesture recognition performance and (2) probe user perceptions and potential applications. Skinergy achieves 92.8 percent accuracy for a 11-class gesture recognition task. Our findings reveal that human factors (e.g., individual differences in skin properties, and aesthetic preferences) are key considerations in designing self-powered on-skin sensors for human inputs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/RobotSweaterThumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/RobotSweaterThumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/RobotSweaterThumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/RobotSweaterThumbnail.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="RobotSweaterThumbnail.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="RobotSweater" class="col-sm-8"> <div class="title">RobotSweater: Scalable, Generalizable, and Customizable Machine-Knitted Tactile Skins for Robots</div> <div class="author"> <a href="https://si-lynnn.github.io/" rel="external nofollow noopener" target="_blank">Zilin Si*</a>, <em>Tianhong Catherine Yu*</em>, Katrene Morozov, <a href="https://www.cs.cmu.edu/~jmccann/" rel="external nofollow noopener" target="_blank">James McCann</a>, and <a href="http://robotouch.ri.cmu.edu/yuanwz/" rel="external nofollow noopener" target="_blank">Wenzhen Yuan</a> </div> <div class="periodical"> <em>In ICRA</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ICRA48891.2023.10161321" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://youtu.be/M1OFmqBRIB8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">video</a> <a href="https://labs.ri.cmu.edu/robotouch/robotsweater/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Tactile sensing is essential for robots to perceive and react to the environment. However, it remains a challenge to make large-scale and flexible tactile skins on robots. Industrial machine knitting provides solutions to manufacture customiz-able fabrics. Along with functional yarns, it can produce highly customizable circuits that can be made into tactile skins for robots. In this work, we present RobotSweater, a machine-knitted pressure-sensitive tactile skin that can be easily applied on robots. We design and fabricate a parameterized multi-layer tactile skin using off-the-shelf yarns, and characterize our sensor on both a flat testbed and a curved surface to show its robust contact detection, multi-contact localization, and pressure sensing capabilities. The sensor is fabricated using a well-established textile manufacturing process with a programmable industrial knitting machine, which makes it highly customizable and low-cost. The textile nature of the sensor also makes it easily fit curved surfaces of different robots and have a friendly appearance. Using our tactile skins, we conduct closed-loop control with tactile feedback for two applications: (1) human lead-through control of a robot arm, and (2) human-robot interaction with a mobile robot.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/uKnitThumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/uKnitThumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/uKnitThumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/uKnitThumbnail.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="uKnitThumbnail.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="uKnit" class="col-sm-8"> <div class="title">uKnit: A Position-Aware Reconfigurable Machine-Knitted Wearable for Gestural Interaction and Passive Sensing using Electrical Impedance Tomography</div> <div class="author"> <em>Tianhong Catherine Yu</em>, <a href="https://rikky0611.github.io/" rel="external nofollow noopener" target="_blank">Riku Arakawa</a>, <a href="https://www.cs.cmu.edu/~jmccann/" rel="external nofollow noopener" target="_blank">James McCann</a>, and <a href="http://www.mayankgoel.com/" rel="external nofollow noopener" target="_blank">Mayank Goel</a> </div> <div class="periodical"> <em>In CHI</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3544548.3580692" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://youtu.be/uPu7JEenUWk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">video</a> <a href="https://smashlab.io/publications/uknit/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>A scarf is inherently reconfigurable: wearers often use it as a neck wrap, a shawl, a headband, a wristband, and more. We developed uKnit, a scarf-like soft sensor with scarf-like reconfigurability, built with machine knitting and electrical impedance tomography sensing. Soft wearable devices are comfortable and thus attractive for many human-computer interaction scenarios. While prior work has demonstrated various soft wearable capabilities, each capability is device- and location-specific, being incapable of meeting users’ various needs with a single device. In contrast, uKnit explores the possibility of one-soft-wearable-for-all. We describe the fabrication and sensing principles behind uKnit, demonstrate several example applications, and evaluate it with 10-participant user studies and a washability test. uKnit achieves 88.0percent/78.2percent accuracy for 5-class worn-location detection and 80.4percent/75.4percent accuracy for 7-class gesture recognition with a per-user/universal model. Moreover, it identifies respiratory rate with an error rate of 1.25 bpm and detects binary sitting postures with an average accuracy of 86.2 percent.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/knitoutVisualizerThumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/knitoutVisualizerThumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/knitoutVisualizerThumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/knitoutVisualizerThumbnail.jpeg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="knitoutVisualizerThumbnail.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="KnitoutVisualizer" class="col-sm-8"> <div class="title">Coupling Programs and Visualization for Machine Knitting</div> <div class="author"> <em>Tianhong Catherine Yu</em>, and <a href="https://www.cs.cmu.edu/~jmccann/" rel="external nofollow noopener" target="_blank">James McCann</a> </div> <div class="periodical"> <em>In SCF</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3424630.3425410" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://youtu.be/SCjqPyj9WUc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">video</a> <a href="https://textiles-lab.github.io/publications/2020-knitout-visualizer/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://github.com/textiles-lab/knitout-live-visualizer" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://textiles-lab.github.io/knitout-live-visualizer/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Try it</a> </div> <div class="abstract hidden"> <p>To effectively program knitting machines, like any fabrication machine, users must be able to place the code they write in correspondence with the output the machine produces. This mapping is used in the code-to-output direction to understand what their code will produce, and in the output-to-code direction to debug errors in the finished product. In this paper, we describe and demonstrate an interface that provides two-way coupling between high- or low-level knitting code and a topological visualization of the knitted output. Our system allows the user to locate the knitting machine operations generated by any selected code, as well as the code that generates any selected knitting machine operation. This link between the code and visualization has the potential to reduce the time spent in design, implementation, and debugging phases, and save material costs by catching errors before actually knitting the object. We show examples of patterns designed using our tool and describe common errors that the tool catches when used in an academic lab setting and an undergraduate course.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Catherine Tianhong Yu. Last updated: October 17, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>