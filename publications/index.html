<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Catherine Yu 于天泓</title> <meta name="author" content="Catherine Tianhong Yu"> <meta name="description" content="\* equal contribution"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?ebb54d8b4c049703faa046e95fe91f77"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/unicorn-logo-zoomed.jpg?3aba6e4cfc5e24bc6e5e03b48b0eb7d7"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="catherineyu.com/publications/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Catherine Yu 于天泓</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/littlething/">LittleThings</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">\* equal contribution</p> </header> <article> <h3 id="peer-reviewed-conference-and-journal-papers">Peer-Reviewed Conference and Journal Papers</h3> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Grab-n-Go_Thumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Grab-n-Go_Thumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Grab-n-Go_Thumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/Grab-n-Go_Thumbnail.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Grab-n-Go_Thumbnail.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Grab-n-Go" class="col-sm-8"> <div class="title">Grab-n-Go: On-the-Go Microgesture Recognition with Objects in Hand</div> <div class="author"> Chi-Jung Lee, Jiaxin Li, <em>Tianhong Catherine Yu</em>, Ruidong Zhang, Vipin Gunda, François Guimbretière, and Cheng Zhang</div> <div class="periodical"> <em>In IMWUT</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3749469" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://youtu.be/VkHmQk3Qm5s?si=D6CPDBmSwDRjEwxG" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">video</a> </div> <div class="abstract hidden"> <p>As computing devices become increasingly integrated into daily life, there is a growing need for intuitive, always-available interaction methods — even when users’ hands are occupied. In this paper, we introduce Grab-n-Go, the first wearable device that leverages active acoustic sensing to recognize subtle hand microgestures while holding various objects. Unlike prior systems that focus solely on free-hand gestures or basic hand-object activity recognition, Grab-n-Go simultaneously captures information about hand microgestures, grasping poses, and object geometries using a single wristband, enabling the recognition of fine-grained hand movements occurring within activities involving occupied hands. A deep learning framework processes these complex signals to identify 30 distinct microgestures, with 6 microgestures for each of the 5 grasping poses. In a user study with 10 participants and 25 everyday objects, Grab-n-Go achieved an average recognition accuracy of 92.0%. A follow-up study further validated Grab-n-Go’s robustness against 10 more challenging, deformable objects. These results underscore the potential of Grab-n-Go to provide seamless, unobtrusive interactions without requiring modifications to existing objects. The complete dataset, comprising data from 18 participants performing 30 microgestures with 35 distinct objects, is publicly available at https://github.com/cjlisalee/Grab-n-Go_Data with the DOI: https://doi.org/10.7298/7kbd-vv75.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/SpellRingThumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/SpellRingThumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/SpellRingThumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/SpellRingThumbnail.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="SpellRingThumbnail.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="SpellRing" class="col-sm-8"> <div class="title">SpellRing: Recognizing Continuous Fingerspelling in American Sign Language using a Ring</div> <div class="author"> Hyunchul Lim, Nam Anh Dang, Dylan Lee, <em>Tianhong Catherine Yu</em>, Jane Lu, Franklin Mingzhe Li, Yiqi Jin, Yan Ma, Xiaojun Bi, Francois Guimbretiere, and Cheng Zhang</div> <div class="periodical"> <em>In CHI</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3706598.3713721" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://youtu.be/CjDHJlpV00Q?si=pFRxvgwgU7dHK1Cw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">video</a> </div> <div class="abstract hidden"> <p>Fingerspelling is a critical part of American Sign Language (ASL) recognition and has become an accessible optional text entry method for Deaf and Hard of Hearing (DHH) individuals. In this paper, we introduce SpellRing, a single smart ring worn on the thumb that recognizes words continuously fingerspelled in ASL. SpellRing uses active acoustic sensing (via a microphone and speaker) and an inertial measurement unit (IMU) to track handshape and movement, which are processed through a deep learning algorithm using Connectionist Temporal Classification (CTC) loss. We evaluated the system with 20 ASL signers (13 fluent and 7 learners), using the MacKenzie-Soukoref Phrase Set of 1,164 words and 100 phrases. Offline evaluation yielded top-1 and top-5 word recognition accuracies of 82.45% (±9.67%) and 92.42% (±5.70%), respectively. In real-time, the system achieved a word error rate (WER) of 0.099 (±0.039) on the phrases. Based on these results, we discuss key lessons and design implications for future minimally obtrusive ASL recognition wearables.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/SeamFitThumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/SeamFitThumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/SeamFitThumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/SeamFitThumbnail.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="SeamFitThumbnail.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="SeamFit" class="col-sm-8"> <div class="title">SeamFit: Towards Practical Smart Clothing for Automatic Exercise Logging</div> <div class="author"> <em>Tianhong Catherine Yu</em>, Manru Mary Zhang*, Luis Miguel Malenab*, Chi-Jung Lee, Jacky Hao Jiang, Ruidong Zhang, Francois Guimbretiere, and Cheng Zhang</div> <div class="periodical"> <em>In IMWUT</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3712287" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> </div> <div class="abstract hidden"> <p>Smart clothing has exhibited impressive body pose/movement tracking capabilities while preserving the soft, comfortable, and familiar nature of clothing. For practical everyday use, smart clothing should (1) be available in a range of sizes to accommodate different fit preferences, and (2) be washable to allow repeated use. In SeamFit, we demonstrate washable T-shirts, embedded with capacitive seam electrodes, available in three different sizes, for exercise logging. Our T-shirt design, customized signal processing &amp; machine learning pipeline allow the SeamFit system to generalize across users, fits, and wash cycles. Prior wearable exercise logging solutions, which often attach a miniaturized sensor to a body location, struggle to track exercises that mainly involve other body parts. SeamFit T-shirt naturally covers a large area of the body and still tracks exercises that mainly involve uncovered joints (e.g., elbows and the lower body). In a user study with 15 participants performing 14 exercises, SeamFit detects exercises with an accuracy of 89%, classifies exercises with an accuracy of 93.4%, and counts exercises with an error of 0.9 counts, on average. SeamFit is a step towards practical smart clothing for everyday uses.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ring-a-pose-thumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ring-a-pose-thumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ring-a-pose-thumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/ring-a-pose-thumbnail.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ring-a-pose-thumbnail.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ring-a-Pose" class="col-sm-8"> <div class="title">Ring-a-Pose: A Ring for Continuous Hand Pose Tracking</div> <div class="author"> <em>Tianhong Catherine Yu</em>, Guilin Hu, Ruidong Zhang, Hyunchul Lim, Saif Mahmud, Chi-Jung Lee, Ke Li, Devansh Agarwal, Shuyang Nie, Jinseok Oh, Francois Guimbretiere, and Cheng Zhang</div> <div class="periodical"> <em>In IMWUT</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3699741" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://youtu.be/j8S0u9Igebc?si=gUAU4DWdl7fO-CnE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">video</a> </div> <div class="abstract hidden"> <p>We present Ring-a-Pose, a single untethered ring that tracks continuous 3D hand poses. Located in the center of the hand, the ring emits an inaudible acoustic signal that each hand pose reflects differently. Ring-a-Pose imposes minimal obtrusions on the hand, unlike multi-ring or glove systems. It is not affected by the choice of clothing that may cover wrist-worn systems. In a series of three user studies with a total of 36 participants, we evaluate Ring-a-Pose’s performance on pose tracking and micro-finger gesture recognition. Without collecting any training data from a user, Ring-a-Pose tracks continuous hand poses with a joint error of 14.1mm. The joint error decreases to 10.3mm for fine-tuned user-dependent models. Ring-a-Pose recognizes 7-class micro-gestures with a 90.60% and 99.27% accuracy for user-independent and user-dependent models, respectively. Furthermore, the ring exhibits promising performance when worn on any finger. Ring-a-Pose enables the future of smart rings to track and recognize hand poses using relatively low-power acoustic sensing.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/SeamPoseThumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/SeamPoseThumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/SeamPoseThumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/SeamPoseThumbnail.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="SeamPoseThumbnail.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="SeamPose" class="col-sm-8"> <div class="title">SeamPose: Repurposing Seams as Capacitive Sensors in a Shirt for Upper-Body Pose Tracking</div> <div class="author"> <em>Tianhong Catherine Yu</em>, Manru Mary Zhang*, Peter He*, Chi-Jung Lee, Cassidy Cheesman, Saif Mahmud, Ruidong Zhang, Francois Guimbretiere, and Cheng Zhang</div> <div class="periodical"> <em>In UIST</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/3654777.3676341" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://youtu.be/KBBBOOAVwes" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">video</a> </div> <div class="abstract hidden"> <p>Seams are areas of overlapping fabric formed by stitching two or more pieces of fabric together in the cut-and-sew apparel manufacturing process. In SeamPose, we repurposed seams as capacitive sensors in a shirt for continuous upper-body pose estimation. Compared to previous all-textile motion-capturing garments that place the electrodes on the clothing surface, our solution leverages existing seams inside of a shirt by machine-sewing insulated conductive threads over the seams. The unique invisibilities and placements of the seams afford the sensing shirt to look and wear similarly as a conventional shirt while providing exciting pose-tracking capabilities. To validate this approach, we implemented a proof-of-concept untethered shirt with 8 capacitive sensing seams. With a 12-participant user study, our customized deep-learning pipeline accurately estimates the relative (to the pelvis) upper-body 3D joint positions with a mean per joint position error (MPJPE) of 6.0 cm. SeamPose represents a step towards unobtrusive integration of smart clothing for everyday pose estimation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/EchoWrist-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/EchoWrist-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/EchoWrist-1400.webp"></source> <img src="/assets/img/publication_preview/EchoWrist.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="EchoWrist.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="EchoWrist" class="col-sm-8"> <div class="title">EchoWrist: Continuous Hand Pose Tracking and Hand-Object Interaction Recognition Using Low-Power Active Acoustic Sensing On a Wristband</div> <div class="author"> Chi-Jung Lee*, Ruidong Zhang*, Devansh Agarwal, <em>Tianhong Catherine Yu</em>, Vipin Gunda, Oliver Lopez, James Kim, Sicheng Yin, Boao Dong, Ke Li, Mose Sakashita, Francois Guimbretiere, and Cheng Zhang</div> <div class="periodical"> <em>In CHI</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3613904.3642910" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://youtu.be/yCLDzKarIEo?si=clRCOeTx818PJtPK" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">video</a> </div> <div class="abstract hidden"> <p>Our hands serve as a fundamental means of interaction with the world around us. Therefore, understanding hand poses and interaction contexts is critical for human-computer interaction (HCI). We present EchoWrist, a low-power wristband that continuously estimates 3D hand poses and recognizes hand-object interactions using active acoustic sensing. EchoWrist is equipped with two speakers emitting inaudible sound waves toward the hand. These sound waves interact with the hand and its surroundings through reflections and diffractions, carrying rich information about the hand’s shape and the objects it interacts with. The information captured by the two microphones goes through a deep learning inference system that recovers hand poses and identifies various everyday hand activities. Results from the two 12-participant user studies show that EchoWrist is effective and efficient at tracking 3D hand poses and recognizing hand-object interactions. Operating at 57.9 mW, EchoWrist can continuously reconstruct 20 3D hand joints with MJEDE of 4.81 mm and recognize 12 naturalistic hand-object interactions with 97.6% accuracy.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/SkinergyThumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/SkinergyThumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/SkinergyThumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/SkinergyThumbnail.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="SkinergyThumbnail.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Skinergy" class="col-sm-8"> <div class="title">Skinergy: Machine-Embroidered Silicone-Textile Composites as On-Skin Self-Powered Input Sensors</div> <div class="author"> <em>Tianhong Catherine Yu</em>, Nancy Wang*, Sarah Ellenbogen*, and Cindy Hsin-Liu Kao</div> <div class="periodical"> <em>In UIST</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3586183.3606729" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://youtu.be/ZXRZnPM9Rfc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">video</a> <a href="https://www.hybridbody.human.cornell.edu/skinergy" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>We propose Skinergy for self-powered on-skin input sensing, a step towards prolonged on-skin device usages. In contrast to prior on-skin gesture interaction sensors, Skinergy’s sensor operation does not require external power. Enabled by the triboelectric nanogenerator (TENG) phenomenon, the machine-embroidered silicone-textile composite sensor converts mechanical energy from the input interaction into electrical energy. Our proof-of-concept untethered sensing system measures the voltages of generated electrical signals which are then processed for a diverse set of sensing tasks: discrete touch detection, multi-contact detection, contact localization, and gesture recognition. Skinergy is fabricated with off-the-shelf materials. The aesthetic and functional designs can be easily customized and digitally fabricated. We characterize Skinergy and conduct a 10-participant user study to (1) evaluate its gesture recognition performance and (2) probe user perceptions and potential applications. Skinergy achieves 92.8 percent accuracy for a 11-class gesture recognition task. Our findings reveal that human factors (e.g., individual differences in skin properties, and aesthetic preferences) are key considerations in designing self-powered on-skin sensors for human inputs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/RobotSweaterThumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/RobotSweaterThumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/RobotSweaterThumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/RobotSweaterThumbnail.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="RobotSweaterThumbnail.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="RobotSweater" class="col-sm-8"> <div class="title">RobotSweater: Scalable, Generalizable, and Customizable Machine-Knitted Tactile Skins for Robots</div> <div class="author"> Zilin Si*, <em>Tianhong Catherine Yu*</em>, Katrene Morozov, James McCann, and Wenzhen Yuan</div> <div class="periodical"> <em>In ICRA</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ICRA48891.2023.10161321" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://youtu.be/M1OFmqBRIB8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">video</a> <a href="https://labs.ri.cmu.edu/robotouch/robotsweater/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Tactile sensing is essential for robots to perceive and react to the environment. However, it remains a challenge to make large-scale and flexible tactile skins on robots. Industrial machine knitting provides solutions to manufacture customiz-able fabrics. Along with functional yarns, it can produce highly customizable circuits that can be made into tactile skins for robots. In this work, we present RobotSweater, a machine-knitted pressure-sensitive tactile skin that can be easily applied on robots. We design and fabricate a parameterized multi-layer tactile skin using off-the-shelf yarns, and characterize our sensor on both a flat testbed and a curved surface to show its robust contact detection, multi-contact localization, and pressure sensing capabilities. The sensor is fabricated using a well-established textile manufacturing process with a programmable industrial knitting machine, which makes it highly customizable and low-cost. The textile nature of the sensor also makes it easily fit curved surfaces of different robots and have a friendly appearance. Using our tactile skins, we conduct closed-loop control with tactile feedback for two applications: (1) human lead-through control of a robot arm, and (2) human-robot interaction with a mobile robot.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/uKnitThumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/uKnitThumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/uKnitThumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/uKnitThumbnail.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="uKnitThumbnail.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="uKnit" class="col-sm-8"> <div class="title">uKnit: A Position-Aware Reconfigurable Machine-Knitted Wearable for Gestural Interaction and Passive Sensing using Electrical Impedance Tomography</div> <div class="author"> <em>Tianhong Catherine Yu</em>, Riku Arakawa, James McCann, and Mayank Goel</div> <div class="periodical"> <em>In CHI</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3544548.3580692" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://youtu.be/uPu7JEenUWk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">video</a> <a href="https://smashlab.io/publications/uknit/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>A scarf is inherently reconfigurable: wearers often use it as a neck wrap, a shawl, a headband, a wristband, and more. We developed uKnit, a scarf-like soft sensor with scarf-like reconfigurability, built with machine knitting and electrical impedance tomography sensing. Soft wearable devices are comfortable and thus attractive for many human-computer interaction scenarios. While prior work has demonstrated various soft wearable capabilities, each capability is device- and location-specific, being incapable of meeting users’ various needs with a single device. In contrast, uKnit explores the possibility of one-soft-wearable-for-all. We describe the fabrication and sensing principles behind uKnit, demonstrate several example applications, and evaluate it with 10-participant user studies and a washability test. uKnit achieves 88.0percent/78.2percent accuracy for 5-class worn-location detection and 80.4percent/75.4percent accuracy for 7-class gesture recognition with a per-user/universal model. Moreover, it identifies respiratory rate with an error rate of 1.25 bpm and detects binary sitting postures with an average accuracy of 86.2 percent.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/knitoutVisualizerThumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/knitoutVisualizerThumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/knitoutVisualizerThumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/knitoutVisualizerThumbnail.jpeg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="knitoutVisualizerThumbnail.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="KnitoutVisualizer" class="col-sm-8"> <div class="title">Coupling Programs and Visualization for Machine Knitting</div> <div class="author"> <em>Tianhong Catherine Yu</em>, and James McCann</div> <div class="periodical"> <em>In SCF</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3424630.3425410" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://youtu.be/SCjqPyj9WUc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">video</a> <a href="https://textiles-lab.github.io/publications/2020-knitout-visualizer/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://github.com/textiles-lab/knitout-live-visualizer" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://textiles-lab.github.io/knitout-live-visualizer/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Try it</a> </div> <div class="abstract hidden"> <p>To effectively program knitting machines, like any fabrication machine, users must be able to place the code they write in correspondence with the output the machine produces. This mapping is used in the code-to-output direction to understand what their code will produce, and in the output-to-code direction to debug errors in the finished product. In this paper, we describe and demonstrate an interface that provides two-way coupling between high- or low-level knitting code and a topological visualization of the knitted output. Our system allows the user to locate the knitting machine operations generated by any selected code, as well as the code that generates any selected knitting machine operation. This link between the code and visualization has the potential to reduce the time spent in design, implementation, and debugging phases, and save material costs by catching errors before actually knitting the object. We show examples of patterns designed using our tool and describe common errors that the tool catches when used in an academic lab setting and an undergraduate course.</p> </div> </div> </div> </li></ol> </div> <h3 id="lbws-workshops">LBWs, Workshops</h3> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/SoftWearablesThumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/SoftWearablesThumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/SoftWearablesThumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/SoftWearablesThumbnail.jpeg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="SoftWearablesThumbnail.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="workshopUbiComp" class="col-sm-8"> <div class="title">Intelligent Soft Wearables</div> <div class="author"> <em>Tianhong Catherine Yu</em>, Cedric Honnet, Tingyu Cheng, Ryo Takahashi, Bo Zhou, Cheng Zhang, Paul Lukowicz, Yoshihiro Kawahara, Josiah Hester, Joseph A Paradiso, Yiyue Luo, and Irmandy Wicaksono</div> <div class="periodical"> <em>In UbiComp Workshop</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3714394.3750561" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://softwearables.github.io/ubicomp25/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Human bodies are almost always in contact with soft materials like clothing, for warmth, protection, self-expression, etc. Recent advancements in intelligent soft wearables have augmented these on-body soft objects with computational functions and intelligence with little compromise on the softness and comforts of wearables, allowing prolonged wear. These innovations, which combine advanced soft sensor design, fabrication, and computational power, offer unprecedented opportunities to improve our health, productivity, and overall well-being with monitoring and assistive capabilities. However, the inherent physical properties of soft materials present unique challenges in achieving practical interactions. The complexity of intelligent soft wearables, multiplexing intricate designs, soft materials, flexible electronics, advanced signal processing algorithms, and machine learning models, necessitates collaborative efforts from experts across diverse domains. This workshop aims to bring together interested researchers and practitioners across relevant domains to discuss the challenges and opportunities of intelligent soft wearables.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/BaseballThumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/BaseballThumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/BaseballThumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/BaseballThumbnail.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="BaseballThumbnail.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Baseball" class="col-sm-8"> <div class="title">Million Eyes on the ’Robot Umps’: The Case for Studying Sports in HRI Through Baseball</div> <div class="author"> Waki Kamino, Andrea W. Wen-Yi, Dhruv Agarwal, Sil Hamilton, Eun Jeong Kang, Jieun Kim, Keigo Kusumegi, Pegah Moradi, Daniel Mwesigwa, Yan Tao, I-Ting Tsai, Ethan Yang, Shengqi Zhu, Shu-Jung Han, Chi-Jung Lee, Michael Joseph Sack, <em>Tianhong Catherine Yu</em>, Weslie Khoo, Andy Elliot Ricci, Yoyo Tsung-Yu Hou, Boyoung Kim, Selma šabanović, David J. Crandall, Karen Levy, and Malte F. Jung</div> <div class="periodical"> <em>In HRI LBW</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.5555/3721488.3721684" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> </div> <div class="abstract hidden"> <p>In this position paper, we argue that baseball-and sports more broadly-provide a unique and under-explored opportunity for researchers to study human-robot interaction (HRI) in real-world settings. Using the rise of robot umpires in baseball as a primary example, we examine emerging themes such as power dynamics among players and umpires, labor implications, and technical challenges. We emphasize the affordances and benefits of studying sports within HRI, including the integration of interdisciplinary perspectives, the large-scale deployment of robots, and the examination of their role in deeply rooted cultural practices.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/SoftWearablesThumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/SoftWearablesThumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/SoftWearablesThumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/SoftWearablesThumbnail.jpeg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="SoftWearablesThumbnail.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="workshopUIST" class="col-sm-8"> <div class="title">Democratizing Intelligent Soft Wearables</div> <div class="author"> Cedric Honnet, <em>Tianhong Catherine Yu</em>, Irmandy Wicaksono, Tingyu Cheng, Andreea Danielescu, Cheng Zhang, Stefanie Mueller, Joe Paradiso, and Yiyue Luo</div> <div class="periodical"> <em>In UIST Workshop</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3672539.3686707" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">doi</a> <a href="https://softwearables.github.io/uist24/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Wearables have long been integral to human culture and daily life. Recent advances in intelligent soft wearables have dramatically transformed how we interact with the world, enhancing our health, productivity, and overall well-being. These innovations, combining advanced sensor design, fabrication, and computational power, offer unprecedented opportunities for monitoring, assistance, and augmentation. However, the benefits of these advancements are not yet universally accessible. Economic and technical barriers often limit the reach of these technologies to domain-specific experts. There is a growing need for democratizing intelligent wearables that are scalable, seamlessly integrated, customized, and adaptive. By bringing researchers from relevant disciplines together, this workshop aims to identify the challenges and investigate opportunities for democratizing intelligent soft wearables within the HCI community via interactive demos, invited keynotes, and focused panel discussions.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Catherine Tianhong Yu. Last updated: February 18, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>